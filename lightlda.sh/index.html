<html> <head>
<title>lightlda.sh</title>
</head>

<body>
<h1>
lightlda.sh: a simple wrapper script for LightLDA.
</h1>
Daichi Mochihashi<br>
The Institute of Statistical Mathematics<br>
$Id: index.html,v 1.2 2018/10/20 05:05:53 daichi Exp $

<h2>Introduction</h2>

lightlda.sh is a package of a wrapper script for
<a href="https://github.com/Microsoft/LightLDA">LightLDA</a>
(Yuan+, WWW 2015)
that can be used easily.<br>
LightLDA is a quite useful program for efficiently training huge topic models,
thanks to a number of mathematical and algorithmic techniques.
However, it employs specific data formats both for the input and the output:
lightlda.sh wraps them to enable using standard input and output formats.

<h2>Content</h2>
<ul>
  <li>lightlda.sh
  <li>lightlda2model.py
  <li>svmlight.py
  <li>train (sample data)
</ul>
Download: <a href="lightlda.sh-0.1.tar.gz">lightlda.sh-0.1.tar.gz</a>

<h2>Install</h2>

<ul>
  <li>First, install LightLDA from:
      <pre>git clone --recursive https://github.com/Microsoft/lightlda</pre>
  <li>Place the scripts above to one of the directories in PATH.
  <li>Then edit lightlda.sh to reflect $root to the root of the LightLDA
      package (like /usr/local/lib/lightlda).
</ul>

<h2>Usage</h2>

using lightlda2.sh is simple:
<pre>
% ./lightlda.sh
lightlda.sh: wrapper to execute LightLDA in a standard way.
usage: lightlda.sh K iters train model [alpha] [beta]
$Id: lightlda.sh,v 1.10 2018/10/23 02:57:28 daichi Exp
</pre>

General usage:
<pre>
% lightlda.sh K iters train model [alpha] [beta]
</pre>
<ul>
  <li>K is the number of topics to assume (e.g. 1000)
  <li>iters is the number of MCMC iterations (e.g. 5000)
  <li>train is a data file of standard format (described below)
  <li>model is a name of a directory to store the trained model
  <li>(optional) alpha is the hyperparameter of document-topic Dirichlet
      (default 0.1)
  <li>(optional) beta is the hyperparameter of topic-word Dirichlet
      (default 0.01)
</ul>

Currently, this script does not support distributed training of LightLDA:
for advanced usage, please use the original program directly.

<h3>Example</h3>

For example, using a sample "train" data file included in this package,
you can run lightlda.sh as:
<pre>
% ./lightlda.sh 10 100 train model
alpha = 0.1 beta = 0.01 topics = 10 iters = 100
preparing data at model ..
There are totally 1324 words in the vocabulary
There are maximally totally 16054 tokens in the data set
The number of tokens in the output block is: 16054
Local vocab_size for the output block is: 1323
Elapsed seconds for dump blocks: 0.0150371
docs  = 200
vocab = 1400
size  = 1
executing LightLDA ..
[INFO] [2018-10-23 12:07:46] INFO: block = 0, the number of slice = 1
[INFO] [2018-10-23 12:07:46] Server 0 starts: num_workers=1 endpoint=inproc://server
[INFO] [2018-10-23 12:07:46] Server 0: Worker registratrion completed: workers=1 trainers=1 servers=1
...
[INFO] [2018-10-23 12:08:46] Server 0: Dump model...
[INFO] [2018-10-23 12:08:46] Server 0 closed.
[INFO] [2018-10-23 12:08:47] Rank 0/1: Multiverso closed successfully.
converting to standard parameters..
reading model..
saving model..
done.
finished.
</pre>

<h2>Data formats</h2>

Training data format is almost the same as
<a href="">lda</a> or <a href="http://svmlight.joachims.org">SVMlight</a>
without labels for each line.
Typical data file is as follows:
<blockquote>
<pre>
1:1 2:4 5:2
1:2 3:3 5:1 6:1 7:1
2:4 5:1 7:1
</pre>
</blockquote>
<ul>
  <li>Each line consists of pairs of &lt;feature_id&gt;:&lt;count&gt;.
      Here, <i>feature_id</i> is an integer from 0;
      <i>count</i> is a positive integer.
  <li>&lt;feature_id&gt;:&lt;count&gt; pairs are separated by (possibly
      multiple) white spaces.
      The program is coded to work even if there are any empty lines, but
      it is preferable that there are no such unnecessary lines.
  <li>For a complete specification, please refer to SVMlight's page.
</ul>			      

<p>
Output is stored in the model directory; in addition to the outputs of
LightLDA, a file "model" is created there for easy use.<br>
"model" is a gzipped pickled data that can be loaded in Python
as follows:<br>
<pre>
import gzip
import cPickle as pickle
with gzip.open ("model", "rb") as gf:
    model = pickle.load (gf)
</pre>
Then "model['alpha']" is a scalar alpha parameter
used for training, and "model['beta']" is VxK matrix of beta parameter,
and "model['gamma']" is a NxK matrix of Dirichlet posteriors for each of the
training documents.

<p>
<hr>
<address>daichi&lt;at&gt;ism.ac.jp</address>
<!-- hhmts start -->
Last modified: Tue Oct 23 12:10:27 2018
<!-- hhmts end -->
</body> </html>
