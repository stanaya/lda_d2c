\documentclass[uplatex]{jsarticle}
\和暦
\usepackage[top=25truemm,bottom=25truemm,left=25truemm,right=25truemm]{geometry}
\usepackage[dvipdfmx]{graphicx}
\usepackage{color}
\usepackage{listings}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
\begin{document}

\title{\huge LDAを含むと３種のクエリ情報取得方法について}
\author{木村　健}
\date{\today}
\maketitle

\section{このドキュメントについて}
本ドキュメントでは現状で使用可能なデータ（D系データとする）について、
クエリ情報と個人識別情報から共通する「クラスタ」\footnote{ここでクラスタというのは、
将棋クラスタとか、アニメクラスタとか言う、Twitterなどでの同じ趣味を持つ人の集まりを想定している。}
を抽出する手法について叩き台としての案を３種述べる。
これらの方法のうち一つはLDA(Latent Dirichlet Allocations)を用いた今までの調査の延長の方法である。
これに対しもう一つの方法はNNを用いた、やや連想ネットワーク的な使い方を取り入れて無理やり教師なし学習を
教師あり学習に持っていくタイプの手法である。そして最後にFSA(Finite State Automata)を用いたやや
古典的な手法について提案してみる。旧来のコンピュータサイエンスっぽいやり方とも言える。

三つの方法は同じタスクをこなすことに関して共通しており、入力と出力はほぼ一緒である。
また互いに手法の一部を使わせてあげたり、使わせてもらったりして、複合的な
手法へと変化する可能性を秘めていると筆者は思っている。

LDAを使うもの以外は筆者の貧困な発想もありイマイチだと自分でも感じているので、
忌憚なきアドバイスをいただけたらと思っている。

想定する読者としては過去のLDA.pyについてのドキュメントをある程度（さらりとでも）読んでいて、
LDAとはなんぞや、くらいの知識がある人を想定している。

\section{３種の方法}
まず３種の問題に先立って入力と出力を定めたい。
\begin{description}
\item[入力] 個人匿名化された個人認識idと発されたクエリーの文字列の対の膨大な集合
\item[出力] 個人認識idのリストの集合と、各リストに付随する連想単語などの付随情報（例えば将棋が好きな人なら「将棋」と言う単語）
\end{description}
いかなる方法を使ってもいいので、このクエリーと個人の関係性から複数の話題に敏感な特定層を抽出したい。それが
本タスクの目的である。

\subsection{LDAを用いた方法（Method-I）}
以前から使っている持橋先生のLDA.pyを活用する。
特に入出力を作り変えるわけではないが、特に入力を工夫してLDA.pyの
入力方法にすると同時に出力のうち特定の行列を使い、クラスタを抽出する。

\subsubsection{入力}
まず、クエリーを定式化する。

\begin{displaymath}
q_i = \cup_{j=1}^{L_i} w_{ij}
\end{displaymath}

\begin{displaymath}
Q = \cup_{i=1}^{N} (q_i, u_i)
\end{displaymath}

この時$u_i$にはユーザー識別idが入っているわけだが、そのユーザーが発したクエリーと
Python的言い方をすればタプルになったものの集合と思っていただければいい。

\begin{displaymath}
U = 1, 2, \ldots, U_{max}
\end{displaymath}

今ユーザーが$U_{max}$しかいないとした時に、クエリーの集合$Q$は$U$により
切断されてユーザーごとのクエリー部分集合となる。

\begin{displaymath}
q_j = \cup (q_i, u_i) | \mbox{where: }\/\/  u_i = j 
\end{displaymath}

全てのユーザ$U$について$q_j$を並べたものをLDAの疎行列（WD行列）とする。
ここでWDのうちWは各クエリに属する単語、Dはドキュメントであるが、ここでは
ユーザーとする。

直感的にユーザー$u$のクエリ集合に対応する単語頻度ベクトルであることが
わかると思う。各クエリの単語頻度ベクトル（単語が一つならone hot vector）
について、これをベクトル和を取ったものとなる。

\subsubsection{出力}

LDAの出力は通常はトピックに属する単語群の集合とすることが多い。ただLDAでは様々な
情報が付随して計算され、例えば各LDAにおいて$k=100$をトピック数とすれば、
100個のトピックそれぞれについて各ドキュメント（＝ユーザ）において
どのような分布を持つか、などの情報もある。
\begin{itemize}
\item $\mbox{ND} = R^{D\times K}$ （ドキュメント数xトピック数）
\end{itemize}

ここからユーザ集合（最初の意味でのクラスタ）を抽出する方法は何通りかあると
思うが、ここでは各ユーザについて上位$n$の頻度の高いトピックを抽出しておき
($n=5なら c_{u_1}, c_{u_2}, \ldots, c_{u_5}$をユーザ$u$に対して数え上げる)
全ての$c_{ui}$についてその単語クラスタに属するユーザ$u$のリストを生成する。

ここでクラスタは最初の意味での共通の意味を持つユーザの集合として、
またLDAを計算するときの単位はトピックと言う単語を使い
この二つを識別することにする。

\subsection{NNを用いた方法（Method-II）}
LDAを用いた方法が割と直感的にクラスタとトピックに属するユーザの関係として
綺麗に紐づくのに対し、NNでの解析方法というと、通常はそれを教師あり学習に
落とし込んで解析しなければならず、あまり良い案が思い浮かばなかった。

思いついた手法としては、LSTMによる系列データの入力に対し、系列データの出力を
答えとするようなDNNを考え、これに対して同じユーザのクエリーをお互いに連想できるように
学習する。実際のクラスタの取得に際しては単語を順にLSTMに与えて、連想する単語を芋づる式に
取り出してきて、その単語を発しているユーザの集合からクラスタを計算する。

ただし、データに対して指摘があったように複数クエリーを発しているユーザは少なく、また
複数単語のクエリーを発しているユーザも少ないため、単語のリンクを辿りながら、あらかじめ
用意してある『単語ーユーザ群』の集合を拡張する形で共通の興味をもつクラスタを取得する。


\subsubsection{あるいは大胆な案}
入力としてユーザ全員のone hot vectorを入力とし、クラスタの出現確率を出力とするLSTM RNNネットワークを
考えて、不特定の組み合わせで入力を発火し、出力となる確度についてその入力の組み合わせをクラスタとする場合どのくらいの
確率でクラスタが形成されるかを考える。不特定の組み合わせを試し、確率の高い組み合わせをn-bestで抽出すれば良いと言う
アイデアだが、この確度は1.0（まさにクラスタである）しか学習データがない気がする。

\subsection{FSAを用いた方法（Method-III）}
少し確率的現象の把握を離れて、もっとグラフ理論的にクラスタを求めることはできないだろうか、
みたいなことを少し考えていて、例えばクエリーの単語$w$をノードとし、その間を単語の進行方向にエッジを張り
最終的に全クエリーで一種の蜘蛛の巣のようなグラフ構造を作り、各単語のノードにその単語を発した
ユーザ群をぶら下げておいて、このグラフのエッジが双方向とした場合のdisjoint sets について
クラスタとして抽出する。

\begin{lstlisting}
WORD_PRE-----WORD------WORD_POST
              U0
              U1
              U2
              ...
              UN
\end{lstlisting}

\begin{thebibliography}{99}
  \bibitem{murphy} Kevin P. Murphy: Machine Learning: A Probabilistic Perspective (Adaptive Computation and Machine Learning series) The MIT Press, 2012.
  \bibitem{blei} Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.: Latent Dirichlet Allocation,
J. Mach. Learn. Res. 3/1, volume 3, 2003.
\end{thebibliography}
\end{document}
