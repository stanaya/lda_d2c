\documentclass[uplatex]{jsarticle}
\和暦
\usepackage[top=25truemm,bottom=25truemm,left=25truemm,right=25truemm]{geometry}
\usepackage[dvipdfmx]{graphicx}
\usepackage{color}
\usepackage{listings}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
\begin{document}

\title{\huge LDA.pyの解説}
\author{木村　健}
\date{\today}
\maketitle


% 以下改善点
% kaz [3:17 PM]
% @kimrin ドキュメントをざっと確認しました。

% コードの解説は、細かいところまでよく読んでいて良いと思います。以下、改善点です。
% ・最初に、コード全体の流れを説明した方が良いです。大きな部分の説明 → 細かな部分の説明という順番です
% ・疑問点、推測となっている部分をまとめた章を作っておくと良いと思います。

% ちなみに、疑問点として書いてあった、len 関数に numpy array を渡したときの挙動ですが、以下のようになるようです。
% https://stackoverflow.com/questions/43081809/len-of-a-numpy-array-in-python

% 細かい部分は、週末か来週にでも見ていきます。
% Stack Overflow
% len() of a numpy array in python


\section{このドキュメントについて}
本ドキュメントはLDA(Latent Dirichret Allocation)について
基礎的な知識を与えると同時に持橋先生が作成されたLDA.pyのコードについて
解説を与える。

ドキュメントの読者はPythonに詳しいことと基本的な確率・統計、線形代数、微分積分
の知識を有することを前提としているが全部わからなくても読めると思う。

\section{LDAのタスクと入力パラメータ}
LDA\cite{blei}はドキュメント群（ここでは簡単のため英語とする）
の各単語について潜在的トピックを推定する技法である。
いくつのトピックに分けるかは最初に指定する。常に同じ順序でトピックが抽出されると限らない。
unsupervised learningである（教師値を用いない）。

LDAの開始に先立って次のパラメータが必要となる(LDA.pyのindex.htmlより引用加筆)
Gibbs samplingという技法を利用していることを付け加えておく。

\begin{itemize}
\item K: topics: number of topics in LDA
\item N: iters: number of Gibbs iterations
\item $\alpha$: Dirichlet hyperparameter on topics
\item $\beta$: Dirichlet hyperparameter on words
\end{itemize}

Kがトピック数で、はじめに与える。例えば$K=10$とする。
Nはイテレーションの回数でこれを大きくするほどperplexityが改善（小さくなる）するが
ある程度まで行くとあまり動かなくなる。$K$が大きいほど収束しずらくなる。

\section{LDA.pyの大まかな流れ}
\subsection{引数からパラメータを計算}
まず引数からパラメータ(K,Nなど)を計算する。同時にtrainファイルとして入力ファイル名を
特定するのと同時に出力ファイル群のprefixを決定する。

\subsection{入力ファイルの処理}
SVMLightの擬似形式のファイルを読み込む。
各レコードが一つのドキュメントに相当する。
レコードはフィールド群からなり、"単語番号:頻度"という形式をしている。
これを単語番号だけの「擬似平文」を作り出し、同時に
与えるトピックのリストのリストも作る。

\subsection{Gibbs samplingへの入力を決定する}
大まかには各パラメータを決定し、必要なベクトル、行列を準備する。
またCythonで書かれた部分の最初で初期化をするがこれは行列などに
カウント値を入れる初期化をしている

\subsection{Gibbs sampling}
まず特定の単語について、各頻度行列から1だけ頻度を引く（なかったことにする）。
これは更新の式が自分のトピックを除いた式だからである。
そしてその単語に紐づいている古いトピック番号を新しいトピック番号に変えて
（どれが新しいトピックかは確率値から決まる）頻度行列を+1する（戻す）。
これを全ての単語について行い、全て終わって1イテレーションとする。
perplexityを表示しながらこれをiters回数実行する。

\subsection{出力データの出力}
テキスト形式で各行列などをファイルに書き込む。

\subsection{これで終わりです}
プログラムを終了する。

% \section{埋め草}

% \LaTeX ファイルの配置が悪いのでここに埋め草を置く。
% \vspace*{5cm}
% \mbox{}

\section{LDA.pyの簡単な解説（コードを巡る）}


\subsection{main}
コードを追っていく。まずmainから。

\begin{lstlisting}
def main ():
    cls = '\x1b[K'
    shortopts = "K:N:a:b:h"
    longopts = ['topics=','iters=','alpha=','beta=','help']
    K      = 10
    iters  = 1
    alpha  = 0
    beta   = 0.01
\end{lstlisting}

Kのデフォルト値は10、iters(N)のデフォルト値は1である。
index.htmlではK=10, N=100を与えている。

alphaとbetaはディリクレ分布のハイパーパラメータである。
betaはほとんど指定することがないのでデフォルトの0.01である。
alphaは0である場合に限り$50/K$が与えられ、alpha=5となる。
alphaはtopicsのためのハイパーパラメータ、
betaはwordsのためのハイパーパラメータである（後述）。
\begin{lstlisting}
    if alpha == 0:
        alpha = 50.0 / K

    if len(args) == 2:
        train = args[0]
        model = args[1]
    else:
        usage ()

    eprint('LDA: K = %d, iters = %d, alpha = %g, beta = %g' \
            % (K,iters,alpha,beta))
    logging.start (sys.argv, model)
          
\end{lstlisting}

\subsection{ldaload}
一通りargumentsを処理するとtrainファイルの読み込みが始まる。
この読み込みはldadata関数が担当する。

\begin{lstlisting}
    eprintf('loading data.. ')
    W,Z = ldaload (train, K)
\end{lstlisting}

ldadataは小さな関数であるが意義は大きい。
SVMLight形式（もどき）になっているドキュメント単位の単語頻度情報を
読み込み、randtopicで全ての単語に乱数でトピックを（暫定的に）与える。

\begin{lstlisting}
def ldaload (file, K):
    words = fmatrix.plain (file)
    topics = randtopic (words, K)
    return int32(words), int32(topics)
\end{lstlisting}

\subsubsection{fmatrix.plain}

ここでfmatrix.plainについて説明する前にtrainとして想定している
入力フォーマットについて述べる。
一行を各ドキュメントとして、単語番号とその単語の頻度を":"で結合した
フィールドが可変長続く。疎行列を表現するときによく用いられる手法である。

\begin{quote}
word0:value0 word1:value1 ... wordX:valueX

例：
1:10 5:12 ... 1010:2
\end{quote}

つまりSVMLightの疎行列入力について、ラベル（教師値）が欠落している状態である。
ただし$wid=0$をSVMは許容していないが、本フォーマットは許容している。
このため相互運用においては注意が必要である。
もうフォーマットは分かっているのでざっと見ていく。
\begin{lstlisting}
import numpy as np

def plain (file):
    """
    build a plain word sequence for LDA.
    """
    data = []
    with open (file, 'r') as fh:
        for line in fh:
            words = parse (line)
            if len(words) > 0:
                data.append (words)
    return data

def parse (line):
    words = []
    tokens = line.split()
    if len(tokens) > 0:
        for token in tokens:
            [id,cnt] = token.split(':')
            # w = int(id) - 1
            w = int(id)
            c = int(cnt)
            words.extend ([w for x in xrange(c)])
        return words
    else:
        return []
\end{lstlisting}

最終的に返すのはリストのリスト、dataである。
ここで疎行列データは単語番号による「擬似平文」に変換される。
つまりあるフィールド 10:4 があったとすると、これが[10,10,10,10]と展開される。
単語番号の若い順に並んでいることが想定されるので（SVMの標準的なフォーマットでは
ソートされた素性番号が使用される）、これは平文そのものではない。

data[i][j]について、iはドキュメントのインデックスであり、jは単語の位置を
表すインデックスである。data[i][j]で得られるものは単語番号である。

こうしてdataはldaload関数でwordsとして与えられる。

\subsection{ldaload再び}

（再掲）
\begin{lstlisting}
def ldaload (file, K):
    words = fmatrix.plain (file)
    topics = randtopic (words, K)
    return int32(words), int32(topics)

def randtopic (words, K):
    topics = []
    for word in words:
        topic = npr.randint(K, size=len(word))
        topics.append (topic)
    return topics

\end{lstlisting}

randtopic関数はwordsの全ての要素を[0,K)のトピック番号で埋める。
しかもランダムに埋める。なのでseedを指定しないとトピックの初期分布は
毎回異なる。

できたtopicsはwordsと同じ形、ただし中身トピック番号となる。
words, topicsのドキュメントごとの要素はplainなPythonのarray
ではなくnumpy.arrayを使う。つまりnumpy.arrayのpython array
である。これはint32関数（自前）でこの形に修正される。

\subsection{登場人物勢揃い}

\begin{lstlisting}
    W,Z = ldaload (train, K)
    D   = len(W)
    V   = lexicon(W)
    N   = datalen(W)
    NW  = np.zeros ((V,K), dtype=np.int32)
    ND  = np.zeros ((D,K), dtype=np.int32)
    NWsum = np.zeros (K, dtype=np.int32)
    eprint ('documents = %d, lexicon = %d, nwords = %d' % (D,V,N))
\end{lstlisting}

ここでほぼ初期状態で必要なものが揃う。

Wはwords=\verb+[numpy.array(wid0, wid1, ... , wid_n), numpy.array(wid0, ...)]+
Zはランダムで作られた単語あたりのtopics=\verb+[numpy.array(t0, t1, ..., t_n), numpy.array(t0,...)]+
となる。
DはWの長さなのでドキュメントの数となる。

lexiconとdatalenについて説明を加えたい。

\begin{lstlisting}
def lexicon (words):
    v = None
    for word in words:
        if max(word) > v:
            v = max(word)
    return v + 1

def datalen (W):
    n = 0
    for w in W:
        n += len(w)
    return n

def datalen (W):
    return sum (map (lambda x: x.shape[0], W))
\end{lstlisting}

datalenが二つ存在する。どちらが実行されているかは
不明であるがおそらくnumpy版の方が実行される。
やることはWの要素数を数えるだけである。

lexiconは単純な関数で、words内の単語番号で一番大きいものに1を足して返す。
これはwords内に単語番号0の単語があるということだろうか、微妙な誤差が
出そうな気はする。

つまり$V=|V|$語彙数、$N=\mbox{要素数}=\mbox{単語数}$ である。

\begin{lstlisting}
    NW  = np.zeros ((V,K), dtype=np.int32)
    ND  = np.zeros ((D,K), dtype=np.int32)
    NWsum = np.zeros (K, dtype=np.int32)
    eprint ('documents = %d, lexicon = %d, nwords = %d' % (D,V,N))
\end{lstlisting}

もう少しで登場人物が揃う。
\begin{itemize}
\item $\mbox{NW} = R^{V\times K}$ （語彙数xトピック数）
\item $\mbox{ND} = R^{D\times K}$ （ドキュメント数xトピック数）
\item $\mbox{NWsum} = R^K$ （トピック数）
\end{itemize}

注意として、1次元目がrows、2次元目がcolumnsである。

\subsection{80/20\% parts}
こうしてメインルーチンの最も計算量が多いパートへと辿り着く。

\begin{lstlisting}
    # initialize
    eprint('initializing..')
    ldac.init (W, Z, NW, ND, NWsum)

    for iter in xrange(iters):
        elprintf('Gibbs iteration [%2d/%2d] PPL = %.4f' \
                 % (iter+1,iters,ldappl(W,Z,NW,ND,alpha,beta)))
        ldac.gibbs (W, Z, NW, ND, NWsum, alpha, beta)
    eprintf('\n')
\end{lstlisting}

ldacは冒頭でimportされているがこれはCythonのモジュールで、正体は
シェアードライブラリ化されたldac.soである。これがCythonモジュールの場合
Pythonのコードの代わりに実行される。
CythonはPythonにTypeの概念が入っているシンタックスのファイルで、
Cとも連携しやすい形である。
主な用途としては重い処理を高速化するために用いる。

やっていることは最初にldac.initしてそれからiters回（Gibbs samplingsの回数）だけループしながら
毎回perplexityを表示（ldappl関数）した後でldac.gibbs （Gibbs Samplingの正体）を
呼んでいるだけである。

\subsection{Gibbs sampling 初期化}
こうしてメインルーチンの最も計算量が多いパートへと辿り着く。

\begin{lstlisting}
def init (list W,
          list Z,
          np.ndarray[dtype_t,ndim=2] NW not None,
          np.ndarray[dtype_t,ndim=2] ND not None,
          np.ndarray[dtype_t,ndim=1] NWsum not None):
    cdef int D = ND.shape[0]
    cdef int N
    cdef Py_ssize_t w, z
    
    for n in range(D):
        N = len(Z[n])
        for i in range(N):
            w         = W[n][i]
            z         = Z[n][i]
            NW[w,z]  += 1
            ND[n,z]  += 1
            NWsum[z] += 1
\end{lstlisting}    

Dはドキュメント数である。
ループが二重になっている。外側はドキュメント数のループ、inner loopは
ドキュメント$n$に含まれる単語数である。つまりWおよびZの全ての要素を
トラバースする。

inner loop内でwとzとして単語番号とトピック番号をWとZより取り出す。
まず$NW_{w, z}$ をインクリメントする。つまりこの行列にはどの語彙がどのトピックで
何回登場したか、が入っている。

次に$ND_{n, z}$ をインクリメントする。NDはどのドキュメントで、どのトピックが、
どのくらい登場したかのカウント値となる。

最後に$NWsum_{z}$をインクリメントする。これはトピックの数だけ要素があるから、
どのトピックが全体でどれだけ出たかのカウント値となる。

つまり最初に$NW, ND, NWsum$を正しいカウント値に初期化する。

\subsection{Gibbs sampling 本丸}

\begin{lstlisting}
def gibbs (list W,
           list Z,
           np.ndarray[dtype_t,ndim=2] NW not None,
           np.ndarray[dtype_t,ndim=2] ND not None,
           np.ndarray[dtype_t,ndim=1] NWsum not None,
           double alpha, double beta):
    cdef int D = ND.shape[0]
    cdef int K = ND.shape[1]
    cdef int V = NW.shape[0]
    cdef int N
    cdef np.ndarray[ftype_t,ndim=1] p = np.zeros (K)
    cdef np.ndarray[dtype_t,ndim=1] Wn
    cdef np.ndarray[dtype_t,ndim=1] Zn
    cdef Py_ssize_t n, i, j, k, Wni, z, znew
    cdef double total
    cdef np.ndarray[ftype_t,ndim=1] rands
    cdef np.ndarray[dtype_t,ndim=1] seq = np.zeros(D,dtype=np.int32)

    # shuffle
    for n in range(D):
        seq[n] = n
    npr.shuffle (seq)
    
    # sample
    for j in range(D):
        n  = seq[j]
        N  = len(Z[n])
        Zn = Z[n]
        Wn = W[n]
        rands = npr.rand (N)
        for i in range(N):
            Wni         = Wn[i]
            z           = Zn[i]
            NW[Wni,z]  -= 1
            ND[n,z]    -= 1
            NWsum[z]   -= 1

            total = 0.0
            for k in range(K):
                p[k] = (NW[Wni,k] + beta) / (NWsum[k] + V * beta) * \
                       (ND[n,k] + alpha)
                total += p[k]
            
            rands[i] = total * rands[i]
            total = 0.0
            znew  = 0
            for k in range(K):
                total += p[k]
                if rands[i] < total:
                    znew = k
                    break

            Zn[i]          = znew
            NW[Wni,znew]  += 1
            ND[n,znew]    += 1
            NWsum[znew]   += 1

\end{lstlisting}

\begin{quote}
\Large ・・・・・・・なが〜い・・・・・・・・
\end{quote}

少しづつ見ていきたい。ここが核心である。
アイデアとしては頻度行列の帳尻が合うように特定アイテムの頻度をデクリメントして、
確率値より新しいトピックZnを求めて、このトピックで頻度行列をインクリメントして
いる。つまり一個取り出して一個追加している。
これを長い間やり続ければより確かなトピック分類に近く、と言うアイデアである。
（アイデア自体は簡単）

\subsection{Gibbs Sampling(1): 各変数おさらい}

\begin{lstlisting}
def gibbs (list W,
           list Z,
           np.ndarray[dtype_t,ndim=2] NW not None,
           np.ndarray[dtype_t,ndim=2] ND not None,
           np.ndarray[dtype_t,ndim=1] NWsum not None,
           double alpha, double beta):
    cdef int D = ND.shape[0]
    cdef int K = ND.shape[1]
    cdef int V = NW.shape[0]
    cdef int N
    cdef np.ndarray[ftype_t,ndim=1] p = np.zeros (K)
    cdef np.ndarray[dtype_t,ndim=1] Wn
    cdef np.ndarray[dtype_t,ndim=1] Zn
    cdef Py_ssize_t n, i, j, k, Wni, z, znew
    cdef double total
    cdef np.ndarray[ftype_t,ndim=1] rands
    cdef np.ndarray[dtype_t,ndim=1] seq = np.zeros(D,dtype=np.int32)
\end{lstlisting}

ここでは文献\cite{murphy}と照らし合わせながらパラメーターを復習したい。

% $C_{ivk} = \displaystyle\sum_{l=1}^{L_i}{II}(q_{il} = k, y_{il} = v)$

\protect\newpage

\begin{table}[htb]
\begin{tabular}{|l||l|l|} \hline
コード内の名前 & \protect\cite{murphy}内での式 & 説明 \\ \hline\hline
W & $y_{il}$ & ドキュメント$i$の$l$番目単語の単語番号$y_{il}$の入ったリストのリスト \\ \hline
Z & $q_{il}$ & ドキュメント$i$の$l$番目のトピック番号$q_{il}$の入ったリストのリスト \\ \hline
\mbox{} & $C_{ivk}$ &%
ドキュメント$i$、単語番号$v$、トピック$k$のカウント値　\\ \hline
NW(V x K) & $C_{vk} = \displaystyle\sum_{i=1}^{D} C_{ivk}$ &%
単語$v$でトピック$k$の頻度をドキュメントで周辺化した行列 \\ \hline
ND(D x K) & $C_{ik} = \displaystyle\sum_{v=1}^{|V|} C_{ivk}$ &%
ドキュメント$i$でトピック$k$を語彙で周辺化したカウント値の行列 \\ \hline
NWsum(K) & $C_{k} = \displaystyle\sum_{v=1}^{|V|} C_{vk}$ &%
トピック$k$の頻度をドキュメントと語彙で周辺化した配列 \\ \hline
D & $N$ & ドキュメント数 \\ \hline
K & $K$ & トピック数 \\ \hline
V & $|V|$ & 語彙数 \\ \hline
N & $L_i$ & ドキュメント$i$の長さ(単語数) \\ \hline
p[k] & & $p(q_{i,l} = k|q_{−i,l}, y_i, \alpha, \gamma) = \displaystyle\frac{C_{(v=y_{il}),k} + \gamma}{C_{k} + |V|\gamma} 
\left\{C_{ik} + \alpha\right\}$ \\ \hline
Wn & $y_{i, {l=[1:L_i]}}$ & ドキュメント$i$の単語番号配列（長さ$L_i$) \\ \hline
Zn & $q_{i, {l=[1:L_i]}}$ & ドキュメント$i$のトピック番号配列（長さ$L_i$) \\ \hline
n & $i$ & シャッフルされたドキュメント群から抽出されたドキュメント番号$i$ \\ \hline
i & $l$(エル) & ドキュメント内の単語位置$l$ \\ \hline
j & & シャッフルされる前のドキュメント番号$i$ \\ \hline
k & $k$ & トピック番号$k$ \\ \hline
Wni & $y_{il}$ & ドキュメント$i$、単語位置$l$の単語番号$y_{il}$ \\ \hline
z & $q_{il}$ & ドキュメント$i$、単語位置$l$のトピック番号$q_{il}$ \\ \hline
znew & $\hat{q_{il}}?$ & ドキュメント$i$単語位置$l$のより尤もらしいトピック番号 \\ \hline
total(最初) & & $\mbox{} = \displaystyle\sum_{k=1}^{K} p(q_{i,l} = k|q_{−i,l}, y_i, \alpha, \gamma)?$ \\ \hline
total(二番目) & & $\mbox{} = \displaystyle\sum_{k'=1}^{k} p(k')$ \\ \hline
rands & & random samples from a uniform distribution over [0, 1). \\ \hline
rands（ループ内） & & $rands(l) = rands(l)\sum_{k=1}^{K} p(q_{i,l} = k|q_{−i,l}, y_i, \alpha, \gamma)$ \\ \hline
seq & & $0,1,\ldots,N-1$をシャッフルした配列（長さ$N$): うまく言えない。。。 \\ \hline
alpha & $\alpha$ & topicsについてのハイパーパラメーター \\ \hline
beta & $\gamma$ & wordsについてのハイパーパラメーター \\ \hline
\end{tabular}
\end{table}

\subsection{Gibbs Sampling(1): ドキュメントのシャッフル}
ドキュメントはシャッフルする。seqにシャッフルされたドキュメント番号
が入っている。

\subsection{Gibbs Sampling(2): ドキュメントループ}
most outer loopのjのループで$n=seq[j]$としてランダムの順にドキュメントを
抽出している。またこの時ドキュメントに紐づいているデータを取り出す
\begin{lstlisting}
        n  = seq[j]
        N  = len(Z[n])
        Zn = Z[n]
        Wn = W[n]
        rands = npr.rand (N)
\end{lstlisting}
randsにはN個（ドキュメント長）の一様分布からサンプルした確率値が入っていて、
これは後で更新される。

\subsection{Gibbs Sampling(3): 単語位置ループ}
ドキュメント内で単語を最初の単語から最後の単語まで走査する。
\begin{lstlisting}
        for i in range(N):
            Wni         = Wn[i]
            z           = Zn[i]
            NW[Wni,z]  -= 1
            ND[n,z]    -= 1
            NWsum[z]   -= 1

            total = 0.0
\end{lstlisting}

この時、先頭から順に\cite{murphy}で言うところの$y_{il}$と$q_{il}$を求めた後、
関連する頻度行列NW,ND,NWsumから当該アイテムのカウントをデクリメントして取り除いている。
またこれから使うtotal変数を初期化して0.0にしている。これにより$-i$と書いてある$c$からの$i$の
除外が行われる。

\subsection{Gibbs Sampling(4): トピックループ1}
p[k]を求める。後の推測の章で詳しく説明するが、尤度の項を
与えているかのような印象である。
そしてtotalでトピックについて周辺化している。

\begin{equation}
p(k) = \frac{C_{(v=y_{il}),k} + \beta}{C_{k} + |V|\beta} \left\{C_{ik} + \alpha\right\}
\end{equation}

が本実装であるが($\beta$は\cite{murphy}では$\gamma$である)、\cite{murphy}には

\begin{equation}
p(q_{i,l} = k|q_{−i,l}, y_i, \alpha, \gamma) ∝ \frac{C_{v,k}^{-} + \gamma}{C_{k}^{-} + |V|\gamma} \frac{C_{i,k}^{-} + \alpha}{L_i + K\alpha}
\end{equation}

とあり、少し次元が違う気がする。具体的には$L_i$に続く項がないため、大きな値になっている。
また、次の記述があり

\begin{quote}
Define $c^{−}_{ivk}$ to be the same as $c_{ivk}$ except it is compute by summing over all locations in document $i$ except for $q_{il}$.\cite{murphy}
\end{quote}

$q_{il}$の分だけ$-1$する必要があることが分かる。 

\subsection{Gibbs Sampling(5): トピックループ2}
\begin{equation}
X_l = P_l[0] \times P_l[1]
\end{equation}
rands[i]は一様分布の乱数に足して次元を合わせるためにtotal($p[k]$の和)を掛けている。
何回も掛けていくのかと思ったが、ループ内で一回だけである。
これは後のp[k]の足し算の時に次元を合わせるためである。
\begin{lstlisting}
            rands[i] = total * rands[i]
            total = 0.0
            znew  = 0
            for k in range(K):
                total += p[k]
                if rands[i] < total:
                    znew = k
                    break
\end{lstlisting}
p[k]をnormalizeする代わりにそのまま（次に生まれた）totalに
加算していく。加算値が乱数を超えたら記号（topic znew）のemitである。
znew=0で初期化されており、デフォルトではtopic0が選ばれる。
この時totalは
\begin{equation}
total_k = \sum_{k'=0}^{k} p[k']
\end{equation}
である。

あとは簡単で、znewをZの配列に入れ直し、NW,ND,NWsumを更新しているだけである。

\begin{lstlisting}
            Zn[i]          = znew
            NW[Wni,znew]  += 1
            ND[n,znew]    += 1
            NWsum[znew]   += 1
\end{lstlisting}

このようにして変更されたZは、新しいトピック群を形成する。

\section{perplexityの計算について}
Gibbs Samplingのiterationを回す時、最初にperplexityを求める。
この値はldappl関数で求まる。

\begin{lstlisting}
def ldappl (W,Z,NW,ND,alpha,beta):
    L = datalen(W)
    lik = polyad (ND,alpha) + polyaw (NW,beta)
    return np.exp (- lik / L)

def datalen (W):
    return sum (map (lambda x: x.shape[0], W))
\end{lstlisting}

datalen関数はW（ドキュメント毎の単語のリスト）の単語の個数をカウントアップする。
これを$L_i$の総和として、$L$と置く。

文献\cite{murphy}より、perplexityは、

\begin{equation}
\mbox{Perplexity}(p_{emp}, p) = \exp 
\left(-\frac{1}{N}\sum_{i=1}^{N}\frac{1}{L_i}\sum_{l=1}^{L_i} \log q(y_{il}) \right)
\end{equation}

で求められることがわかる。ただ本実装は違う求め方をしている。

まず$\displaystyle L = \sum_{i=1}^{N} Li$である。
また$N$はドキュメント数である。


\begin{lstlisting}
def gammaln (x):
    return scipy.special.gammaln (x)
\end{lstlisting}

ここで
まずgammalnに注目する。
$exp(gammaln(x)) = gammasgn(x)*gamma(x)$
だそうである。次にもう一度perplexityの定義に戻る。

\begin{lstlisting}
def ldappl (W,Z,NW,ND,alpha,beta):
    L = datalen(W)
    lik = polyad (ND,alpha) + polyaw (NW,beta)
    return np.exp (- lik / L)
\end{lstlisting}

lik（恐らく尤度）はpolyad関数とpolyaw関数の和である。
polyad関数とpolyaw関数を解析しよう。

\begin{lstlisting}
def polyad (ND,alpha):
    D = ND.shape[0]
    K = ND.shape[1]
    nd = np.sum(ND,1)
    lik = np.sum (gammaln (K * alpha) - gammaln (K * alpha + nd))
    for n in xrange(D):
        lik += np.sum (gammaln (ND[n,:] + alpha) - gammaln (alpha))
    return lik
\end{lstlisting}
Dはドキュメント数、Kはトピック数である。（\cite{murphy}の場合NとKである）。
ndはrowごとに和を取っていて、rowはD個なので、各ドキュメントのトピックごとの頻度の和である。
これは各ドキュメントごとの単語数に相当する。
（\cite{murphy}の場合$C_{i} = \sum_{k=1}^{K} C_{ik}$）。

\begin{equation}
lik = \log (\Gamma(K\alpha)) - \log (\Gamma([C_{1} + K\alpha, C_{2} + K\alpha, \ldots, C_{N} + K\alpha))
\end{equation}

またループ内は

\begin{equation}
lik = \sum_{i=1}^{N} \left\{ \log (\Gamma([c_{i,0} + \alpha, c_{i,1} + \alpha, \ldots, c_{i,K} + \alpha\protect])) - \log(\Gamma(\alpha)) \right\}
\end{equation}


同様な形式をしているpolyawも同様の式である。
これらよりperplexityが$\exp(-\frac{lik}{L})$より求まる。

\section{perplexityについて推測の章}
コードと\cite{murphy}の両方から推測するに、次のような関係があるのではないだろうか

\subsection{polyadについて}
\begin{lstlisting}
def polyad (ND,alpha):
    D = ND.shape[0]
    K = ND.shape[1]
    nd = np.sum(ND,1)
    lik = np.sum (gammaln (K * alpha) - gammaln (K * alpha + nd))
    for n in xrange(D):
        lik += np.sum (gammaln (ND[n,:] + alpha) - gammaln (alpha))
    return lik
\end{lstlisting}
（\cite{murphy}の場合$C_{i} = \sum_{k=1}^{K} C_{ik}$）。

\begin{equation}
lik = \log (\Gamma(K\alpha)) - \log (\Gamma([C_{1} + K\alpha, C_{2} + K\alpha, \ldots, C_{N} + K\alpha))
\end{equation}

またループ内は

\begin{equation}
lik = \sum_{i=1}^{N} \left\{ \log (\Gamma([c_{i,0} + \alpha, c_{i,1} + \alpha, \ldots, c_{i,K} + \alpha\protect])) - \log(\Gamma(\alpha)) \right\}
\end{equation}

ここで\cite{murphy}を見てみると、

\begin{equation}
p({\bf q} | \alpha) = \left(\frac{\Gamma(K\alpha)}{\Gamma(\alpha)^K}\right)^{N}
\prod_{i=1}^{N} \frac{\prod_{k=1}^{K} \Gamma(C_{ik} + \alpha)}{\Gamma(L_i + K\alpha)}
\end{equation}

(marginal priorだそうである)。

ここで$C_i$ は $L_i$ であることがわかるし、近い式であることがわかる。

\subsection{polyawについて}
コード中のbetaは\cite{murphy}での$\gamma$である。
\begin{lstlisting}
def polyaw (NW,beta):
    V = NW.shape[0]
    K = NW.shape[1]
    nw = np.sum(NW,0)
    lik = np.sum (gammaln (V * beta) - gammaln (V * beta + nw))
    for k in xrange(K):
        lik += np.sum (gammaln (NW[:,k] + beta) - gammaln (beta))
    return lik
\end{lstlisting}

Vは語彙数、Kはトピック数である。nwは$C_k$となる。
文献\cite{murphy}を引用する。

\begin{equation}
p({\bf y} | {\bf q}, \gamma) = \left(\frac{\Gamma(V\gamma)}{\Gamma(\gamma)^V}\right)^{K}
\prod_{k=1}^{K} \frac{\prod_{v=1}^{V} \Gamma(C_{vk} + \gamma)}{\Gamma(C_k + V\gamma)}
\end{equation}

これもまた近い式であることがわかる。

gammalnを使っているため対数が掛かっているが、確率値のlogである。

それを$\exp(-logprobability)$として確率値に戻していると考えられる。

\section{改善案}
\subsection{改善前}
改善前のホットスポットはこんなコードである。
\begin{lstlisting}
            total = 0.0
            for k in range(K):
                p[k] = (NW[Wni,k] + beta) / (NWsum[k] + V * beta) * \
                       (ND[n,k] + alpha)
                total += p[k]
            
            rands[i] = total * rands[i]
            total = 0.0
            znew  = 0
            for k in range(K):
                total += p[k]
                if rands[i] < total:
                    znew = k
                    break

\end{lstlisting}

\section{改善後}

次の改善を提案する。大きな効果はなかったが、
次元が合うなどの作用がある。

\begin{lstlisting}
        for i in range(N):
            Wni         = Wn[i]
            z           = Zn[i]
            NW[Wni,z]  -= 1
            ND[n,z]    -= 1
            NWsum[z]   -= 1
            doc_len = N - 1

            normalizer = 0.0
            for k in range(K):
                p[k] = (ND[n,k] + beta) / (doc_len + beta * K) * (NW[Wni,k] + alpha) / (NWsum[k] + alpha * V)
                normalizer += p[k]

            total = 0.0
            znew  = 0
            for k in range(K):
                total += p[k] / normalizer
                if rands[i] < total:
                    znew = k
                    break
\end{lstlisting}
doc\_lenとして新しい項が$p[k]$に追加されていると同時に、totalでも止める試算値をnormalizeで割っている。
おそらくp[k]が若干正確になっている。

\section{テストセット用に構築したコーパス環境とツール}

\begin{table}[htb]
\begin{tabular}{|l||l|l|} \hline
ツール名 & Pythonのバージョン & 付記 \\ \hline \hline
lda.py & python2 & 持橋先生ツール \\ \hline \hline
preNIPS.py & python2 & NIPSコーパス→svm \\ \hline
preNewsGroups.py & python2 & newsgroup20→svm \\ \hline
preJaText8.py & python3 & ja.text8→svm \\ \hline
visLDA.py & python3 & .wzファイル→結果csv \\ \hline
coolcutter.py & python3 & .svmファイルシュリンクツール \\ \hline
scikit-LDA.py & python3 & scikit-learnでLDA \\ \hline \hline
ExtractCorpus.py & python3 & ja.text8(new) 作成ツール \\ \hline
\end{tabular}
\end{table}
またこれらのツールのうち木村の自作のツールのほとんどは.wordsファイルを副次的に
扱う。

\subsection{使い方、コマンドラインオプションなど}
\subsubsection{preNIPS.py}

usageは次の通り。
\begin{lstlisting}
MacBook-Pro:nlp kimrin$ python preNIPS.py
usage: preNIPS.py NIPS_1987-2015.csv wordsfile.txt
       output sparse matrix file that work for LDA.py
       also put words list file.
MacBook-Pro:nlp kimrin$
\end{lstlisting}
NIPSコーパス自体は一つの大きなcsvファイルである。これはrowが単語、columnがdocumentと
なっている。これを転地し、svmの疎行列もどきに直す。
引数としてNIPSコーパスのcsvファイルと出力されるwordsファイルを指定する。
これからldaするときはNIPSというprefixの付いたファイル群を生成するので、
このときNIPS.wordsをwordsファイルとする。
注意して欲しいのはwordsファイルは1column目をそのまま切り出したものなので、
alphabetical orderになっている点である。他のコーパスは頻度順である。

\begin{lstlisting}
MacBook-Pro:nlp kimrin$ python preNIPS.py ../data/NIPS_1987-2015.csv ../data/NIPS.words
reading 0...
reading 100...
...
reading 11300...
reading 11400...
writing 0...
writing 100...
writing 200...
...
writing 5700...
writing 5800...
generated ../data/NIPS_1987-2015.csv.pseudo.svm and ../data/NIPS.words
next, run LDA.py to obtain latent dirichlet allocations.
\end{lstlisting}
../data/NIPS\_1987-2015.csv.pseudo.svm ファイルが出来上がる。
同時に../data/NIPS.wordsファイルもできあがる。

\subsubsection{preNewsGroups.py}
usageは次の通りである。
\begin{lstlisting}
MacBook-Pro:nlp kimrin$ python preNewsGroups.py
Usage:
  preNewsGroups.py [--pt] <news_groups_dir> <pseudo_svm_file> <prefix>
  preNewsGroups.py (-e | --each) <a_file>
  preNewsGroups.py (-h | --help)
  preNewsGroups.py --version
MacBook-Pro:nlp kimrin$
\end{lstlisting}
20newsgroupsコーパスは一つのドキュメントが一つのファイルに入っている（NetNewsの記事）。
これをrecursiveに集めてきて、svmファイルを作る。
\begin{lstlisting}
MacBook-Pro:nlp kimrin$ python preNewsGroups.py ../data/20news-bydate/ 20news.svm 20news
../data/20news-bydate/20news-bydate-test/talk.politics.mideast
...
../data/20news-bydate/20news-bydate-train/talk.religion.misc
MacBook-Pro:nlp kimrin$
\end{lstlisting}

20news.svmファイルにpseudo svmファイルが、20news.wordsにwordsファイルが格納される。
testとtrainが別れているが両方コーパスとして扱う。wordsファイルのオーダーは頻度オーダーである。

\subsubsection{preJaText8.py}
ja.text8（改良版）コーパスを読み込んでpseudo svmファイルとwordsファイルを出力する。
usageは次の通り。
\begin{lstlisting}
MacBook-Pro:nlp kimrin$ python3 preJaText8.py
Usage:
  preJaText8.py <corpus_file> <prefix>
  preJaText8.py (-h | --help)
  preJaText8.py --version
MacBook-Pro:nlp kimrin$
\end{lstlisting}
python3ファイルである。コーパスファイルとprefixを与える。ja.text8.svm と ja.text8.wordsが出来上がる。

\begin{lstlisting}
MacBook-Pro:nlp kimrin$ python3 preJaText8.py ../ja.text8/ja.text8.20180601.100MB ja.text8
36435 articles are processed.
MacBook-Pro:nlp kimrin$
\end{lstlisting}

\subsubsection{lda.py}
（持橋先生の作成されたシステムである）。
LDAを行い結果を.wzファイルに格納する。.wzファイルは単語番号とトピック番号の対が、
コーパス内の単語の数だけ格納されたテキストファイルである。そのほかハイパーパラメータなども
保存される。
\begin{lstlisting}
MacBook-Pro:nlp kimrin$ python python/lda.py
usage: lda.py OPTIONS train model
OPTIONS
 -K topics  number of topics in LDA
 -N iters   number of Gibbs iterations
 -a alpha   Dirichlet hyperparameter on topics
 -b beta    Dirichlet hyperparameter on words
 -h         displays this help
$Id: lda.py,v 1.10 2016/02/06 14:05:38 daichi Exp $
MacBook-Pro:nlp kimrin$
\end{lstlisting}
trainと書いてあるところに.svmファイルを指定する。modelの代わりにprefixを指定する。
このときprefix.wordsがあらかじめ作られているのなら、prefixをそれに合わせると都合が良い。

\begin{lstlisting}
MacBook-Pro:nlp kimrin$ python python/lda.py -K 10 -N 100 --alpha=0.01 ../data/ja.text8.svm ../data/ja.text8
LDA: K = 10, iters = 100, alpha = 0.01, beta = 0.01
loading data.. documents = 35938, lexicon = 17416, nwords = 8254429
initializing..
100,4384.1300
saving model..
done.
MacBook-Pro:nlp kimrin$
\end{lstlisting}

logをcsvとして読み込むためのパッチが当ててあるので結果が少し違うが、
iterationの数とperplexityが出るようになっている。

\subsubsection{visLDA.py}
lda.pyの結果をわかりやすいcsvファイルの形にする整形ツールである。python3ファイルである。
頻度の高い単語の順に、各コラムにトピックを割り当てる。カッコ内の数字はそのトピック内での頻度である。
いくつか例外があり、$=$はマクロとなってしまうため、コーパス内に$=$があってこれが結果に反映される
場合はマクロを無効化して差し支えない。またcp932（windowsのshift-jis）にない文字が含まれている
場合、これを別の文字で置き換えるが稀である。
usageは次の通りである。

\begin{lstlisting}
MacBook-Pro:nlp kimrin$ python3 visLDA.py
visLDA.py: visualize allocations of LDA.
But this time, only emits one csv file(columns = topics, rows = word rank).

 $ python visLDA.py prefix words.txt
   prefix: corpus name (such as model, NIPS).
   words.txt: word file (for example, NIPS.words)
MacBook-Pro:nlp kimrin$
\end{lstlisting}
まず.wzファイルが含まれているprefixを指定する。同時に.wordsファイルのファイル名を指定する。
本当はprefixだけにしてシンプルなコマンドラインにしたかったができなかった。
出力として、prefix.topics.csvが出力される。いつもExcelで開いているがNumbersでも
開けるかもしれない。$K$の数だけカラムができる。

\begin{lstlisting}
MacBook-Pro:nlp kimrin$ python3 visLDA.py ../data/ja.text8 ../data/ja.text8.words
make bag of words: doc id 100
make bag of words: doc id 200
...
make bag of words: doc id 35900
read 35938 documents. 10 topics.
draw 9366 records.
MacBook-Pro:nlp kimrin$
\end{lstlisting}

\subsubsection{coolcutter.py}
後述のscikit-LDA.pyの前処理に感化されて作った素性削除ツールである。
.svmファイルを入力し、.cool.svmファイルを出力する。.cool.svmファイルが
素性削除されてスリムになったファイルである。
あくまで客観的に言えないが、これを通すと結果がある程度妥当になる。
ただscikit-LDA.pyのところでも述べるが、perplexityはscikit-LDA.pyの
方がかなり低い。

\begin{lstlisting}
MacBook-Pro:nlp kimrin$ python3 coolcutter.py
Usage:
  coolcutter.py <prefix>
  coolcutter.py (-h | --help)
  coolcutter.py --version
\end{lstlisting}

.svmファイルのprefixをコマンドラインに指定する。
\begin{lstlisting}
MacBook-Pro:nlp kimrin$ python3 coolcutter.py ../data/ja.text8
1. calculated df.
2. omit exceed max df and lower df in probability...
3. calculate tf in corpus.
4. sort it
5. perform deletion
write ../data/ja.text8.cool.svm.
\end{lstlisting}


\begin{lstlisting}
MacBook-Pro:nlp kimrin$ ls -l ../data/ja.text8.svm ../data/ja.text8.cool.svm
-rw-r--r--  1 kimrin  staff    303286 Jun 25 20:40 ../data/ja.text8.cool.svm
-rw-r--r--  1 kimrin  staff  28795790 Jun 14 23:34 ../data/ja.text8.svm
MacBook-Pro:nlp kimrin$
\end{lstlisting}

1/100くらいになるまでごっそり素性を削除する。perplexityが改善するが、
scikit-learnのLDAには及ばない。

\subsubsection{scikit-LDA.py}
scikit-learnにLDAがあると聞いてお試しで解析してみた。
あくまでお試しなのでja.text8コーパスがハードコーディングされている。
環境に合わせて変更するか、もしもっと使いたければコマンドラインを拡張して欲しい。
NMFモデルとLDAモデルの両方が試される。LDAはtfしか使わない。入力としてはスペース区切り、
一行１documentの形式で十分である。

前処理が施されている。dfが0.95以上のものは取り除く（確率値として）。dfが2以下のものは
取り除く（頻度として）。素性は1000で。

結果も併せて掲載する。

\begin{verbatim}
MacBook-Pro:nlp kimrin$ python3 scikit-LDA.py
Loading dataset...
done in 1.165s.
Extracting tf-idf features for NMF...
done in 8.229s.
Extracting tf features for LDA...
done in 8.601s.

Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=36436 and n_features=1000...
done in 4.889s.

Topics in NMF model (Frobenius norm):
Topic #0: する こと いる れる ない ある など よう もの ため この から として その という なる 場合 られる また によって
Topic #1: 選手 出場 リーグ 試合 シーズン チーム 大会 サッカー 優勝 移籍 選手権 代表 出身 プロ fc 野球 所属 オリンピック 契約 クラブ
Topic #2: アルバム 発売 シングル リリース 収録 作曲 バンド 楽曲 cd the 音楽 いる オリジナル ライブ レコード から ベスト 発表 歌手 限定
Topic #3: 日本 教授 大学 研究 東京 学校 卒業 高等 出身 博士 名誉 学者 大学院 文学 教育 所属 専門 生まれなど 活動
Topic #4: 放送 番組 テレビ ラジオ 00 から 出演 時間 まで ドラマ nhk 制作 系列 10 30 開始 終了 コーナー情報 担当
Topic #5: ある いる ホーム 鉄道 位置 国道 あり 道路 km 路線 どう バス 現在 一般 から 列車 jr 設置 結ぶする
Topic #6: 小学校 市立 中学校 学校 丁目 高等 大阪 ある こう 平成 教育 する 以下 県立 通り 統合 現在 神戸福岡 住宅
Topic #7: 映画 作品 監督 出演 公開 女優 俳優 製作 主演 ドラマ 日本 テレビ 漫画 受賞 シリーズ デビュー 小説 撮影 出身 いる
Topic #8: から あっ なっ として 時代 現在 られ 昭和 明治 10 まで 12 いる 11 この ため 江戸 なり その により
Topic #9: 人口 イタリア 共和 自治体 基礎 以下 km 通り ある 一つ する 位置 都市 距離 面積 行政 地域 から示す 2000

Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=36436 and n_features=1000...
done in 74.608s.

Topics in NMF model (generalized Kullback-Leibler divergence):
Topic #0: する ある いる れる こと として など また あり ない から なる ため なっ もの この よう その られ によって
Topic #1: 出身 日本 所属 として 活動 から 卒業 東京 12 11 デビュー 同年 高校 務め 代表 その後 参加 10 大学 より
Topic #2: 発売 から 放送 番組 いる シングル アルバム 音楽 収録 まで リリース テレビ 作曲 制作 楽曲 より開始 時間 作品 ラジオ
Topic #3: 日本 学校 東京 高等 教授 研究 など 大学 株式会社 設立 教育 生まれ 専門 現在 卒業 本社 ある 法人 会社 運営
Topic #4: 選手 する 出場 開催 大会 優勝 選手権 チーム 試合 リーグ サッカー 行わ シーズン プロ 獲得 野球から 代表 競技 世界
Topic #5: ある 位置 いる 現在 鉄道 から 市立 小学校 一般 以下 どう 道路 存在 地域 結ぶ 都市 通り 路線 設置 番号
Topic #6: する こと ため その なっ なる この れる られ しかし ない として という よう あっ なかっ これだっ なり また
Topic #7: 日本 映画 作品 ある 監督 いる する 漫画 など アメリカ合衆国 舞台 俳優 英語 連載 女性 公開 小説 男性 出演 による
Topic #8: あっ 現在 時代 として なっ から まで いる 12 10 11 により 昭和 られ 明治 なり 江戸 ため 当時 15
Topic #9: ある 生まれ イタリア 共和 ドイツ 人口 フランス 一つ から 発見 イギリス 基礎 政治 通り アメリカ 世界 自治体 世紀 知ら 以下

Fitting LDA models with tf features, n_samples=36436 and n_features=1000...
iteration: 1 of max_iter: 5, perplexity: 369.1543
iteration: 2 of max_iter: 5, perplexity: 364.6237
iteration: 3 of max_iter: 5, perplexity: 363.0649
iteration: 4 of max_iter: 5, perplexity: 362.1714
iteration: 5 of max_iter: 5, perplexity: 361.5199
done in 147.643s.

Topics in LDA model:
Topic #0: 研究 大学 学校 教授 教育 科学 小惑星 高等 文化 学者 社会 博士 日本 植物 技術 専門 研究所 建築病院 経済
Topic #1: ある いる から する なっ 鉄道 昭和 あっ 平成 まで 現在 設置 として 列車 車両 路線 バス 地区 10 建設
Topic #2: 放送 番組 から 音楽 いる アルバム 出演 テレビ ある として 10 発売 まで シングル 作曲 収録 活動 12 11 ドラマ
Topic #3: 日本 など 映画 いる として 東京 ある 会社 する から 出身 活動 昭和 監督 卒業 設立 株式会社 事業 企業 現在
Topic #4: する ある こと ない について れる 場合 において よう もの その なる という として いる 問題 なら できる 市立 小学校
Topic #5: から する ドイツ なっ として あっ フランス この 政府 ため イギリス 戦争 アメリカ まで 海軍 ロシア こと 主義 部隊 10
Topic #6: から 選手 チーム 試合 出場 大会 シーズン リーグ 優勝 として 代表 なっ 選手権 する 野球 プロ 10 開催 世界 獲得
Topic #7: から こと いる する なっ この ある あっ その として ため という られ ない よう れる だっ 時代なかっ なる
Topic #8: ある いる する れる こと など から この として ない もの ため あり よう られ その また によって られる なる
Topic #9: いる する ある こと から なっ 作品 として など ない ゲーム シリーズ 開発 登場 使用 ため 発売れる また より

MacBook-Pro:nlp kimrin$
\end{verbatim}

\subsubsection{ExtractCorpus.py}
これは今回のLDA用前処理としてはちょっと特殊な部類に入るのであるが、
コーパス作成のためのスクリプトである。

ja.text8をforkし、一行１ドキュメントになるように修正した。
またja.text8が使っていた元のwikipediaのファイルはwikipediaサーバーに
なかったので最新の同様のファイルを使って作成した。
このスクリプトを実行してコーパスを作るのではなく、このリポジトリの
setup.shを実行すると良い塩梅にこのスクリプトが呼ばれる。

基本的にはMeCabで分かち書きし、一つのドキュメントを一つの行に
収める。100MBに達しそうなところでclampする。

\begin{lstlisting}
MacBook-Pro:nlp kimrin$ ls -l ../ja.text8/ja.text8.20180601.100MB
-rw-r--r--  1 kimrin  staff  100036067 Jun 14 11:42 ../ja.text8/ja.text8.20180601.100MB
MacBook-Pro:nlp kimrin$
\end{lstlisting}


\begin{thebibliography}{99}
  \bibitem{murphy} Kevin P. Murphy: Machine Learning: A Probabilistic Perspective (Adaptive Computation and Machine Learning series) The MIT Press, 2012.
  \bibitem{blei} Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.: Latent Dirichlet Allocation,
J. Mach. Learn. Res. 3/1, volume 3, 2003.
\end{thebibliography}
\end{document}